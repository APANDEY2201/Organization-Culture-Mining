{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "05_LDA_03_Training.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XgodfZMtD6uG"
      },
      "source": [
        "Because of parellel processing capability provided by gensim,\n",
        "it is necessary to explicitly mention entire data\n",
        "to be processed in main thread (because data is small).\n",
        "\n",
        "Due to this if condition, notebook cell are not surrounded by text annotations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "99RrVv9-DzvH"
      },
      "source": [
        "if __name__ == '__main__':\n",
        "    from gensim.models import LdaModel, LdaMulticore\n",
        "    from gensim import corpora\n",
        "    import spacy\n",
        "    import xlsxwriter\n",
        "    import datetime\n",
        "    import shutil\n",
        "    import os\n",
        "    import pickle\n",
        "\n",
        "    # For POS tagging and lemmatization\n",
        "    nlpDe = spacy.load('de_core_news_sm')\n",
        "    nlpEn = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "    # For lemmatization\n",
        "    def germanSpacyLemmatizer(token):\n",
        "        token = token.lower()\n",
        "        lemmed = ''\n",
        "        for t in nlpDe.tokenizer(token):\n",
        "            lemmed = lemmed + ' ' + t.lemma_\n",
        "        return lemmed.strip()\n",
        "    def englishSpacyLemmatizer(token):\n",
        "        token = token.lower()\n",
        "        lemmed = ''\n",
        "        for t in nlpEn.tokenizer(token):\n",
        "            lemmed = lemmed + ' ' + t.lemma_\n",
        "        return lemmed.strip()\n",
        "\n",
        "\n",
        "    # For POS tagging\n",
        "    def germanSpacyPOS(token):\n",
        "        return nlpDe(token)[0].pos_\n",
        "    def englishSpacyPOS(token):\n",
        "        return nlpEn(token)[0].pos_\n",
        "\n",
        "    # No. of topics as hyperparameter\n",
        "    topics = [15,16,17,18,19,20,21,22,23,24,25]\n",
        "    pickle.dump(topics, open('dataOCM/02_LDA/LDA_03_Training_noOfTopics.list', 'wb'))\n",
        "\n",
        "    # Saving LDA models with different no. of topics in different folders\n",
        "    now = datetime.datetime.now()\n",
        "    currDateTime = (now.strftime(\"%d%m%Y_%H%M%S\"))\n",
        "    dir = 'dataOCM/02_LDA/LDA_Runs/01_All/' + currDateTime\n",
        "    pickle.dump(dir, open('dataOCM/02_LDA/LDA_03_Training_LastRunTime.string', 'wb'))\n",
        "    if not os.path.exists(dir):\n",
        "        os.makedirs(dir)\n",
        "\n",
        "    # No. of topics loop\n",
        "    for tps in range(len(topics)):\n",
        "        # Some model parameters init\n",
        "        setNoOfTopics = topics[tps]\n",
        "        setAlpha = 0.1\n",
        "        setEta = 0.1\n",
        "        setFitBundle = False\n",
        "        setTempRun = '_engsample'\n",
        "\n",
        "        # Loading necessary pickles\n",
        "        dct = corpora.dictionary.Dictionary.load('dataOCM/02_LDA/LDA_02_Preprocessing_Dictionary.dictionary')\n",
        "        corpus = pickle.load(open('dataOCM/02_LDA/LDA_02_Preprocessing_Corpus.corpus', 'rb'))\n",
        "        keywordsConstructAll = pickle.load(open('dataOCM/02_LDA/LDA_02_Preprocessing_keywordsConstructAllNew.list', 'rb'))\n",
        "        keywordsConstructAllIDsInDct = pickle.load(open('dataOCM/02_LDA/LDA_02_Preprocessing_keywordsConstructAllIDsInDctNew.list', 'rb'))\n",
        "        listNoun = pickle.load(open('dataOCM/02_LDA/LDA_02_Preprocessing_listNounNew.list', 'rb'))\n",
        "        listAdj = pickle.load(open('dataOCM/02_LDA/LDA_02_Preprocessing_listAdjNew.list', 'rb'))\n",
        "        listVerb = pickle.load(open('dataOCM/02_LDA/LDA_02_Preprocessing_listVerbNew.list', 'rb'))\n",
        "        listNounIds = pickle.load(open('dataOCM/02_LDA/LDA_02_Preprocessing_listNounIdsNew.list', 'rb'))\n",
        "        listAdjIds = pickle.load(open('dataOCM/02_LDA/LDA_02_Preprocessing_listAdjIdsNew.list', 'rb'))\n",
        "        listVerbIds = pickle.load(open('dataOCM/02_LDA/LDA_02_Preprocessing_listVerbIdsNew.list', 'rb'))\n",
        "        dctOpsLog = pickle.load(open('dataOCM/02_LDA/LDA_02_Preprocessing_dctOpsLog.list', 'rb'))\n",
        "\n",
        "        # Facility to provide asymetric prior Eta\n",
        "        probSkew=15\n",
        "        # for t in range(noOfTopics):\n",
        "        #     wordProbs_temp = []\n",
        "        #     for d in range(len(dct)):\n",
        "        #         wordProbs_temp.append(10)\n",
        "        #         if t == 0:\n",
        "        #             if dct[d] in keywordsConstruct1:\n",
        "        #                 wordProbs_temp[d] = probSkew\n",
        "        #         if t == 1:\n",
        "        #             if dct[d] in keywordsConstruct2:\n",
        "        #                 wordProbs_temp[d] = probSkew\n",
        "        #         if t == 2:\n",
        "        #             if dct[d] in keywordsConstruct3:\n",
        "        #                 wordProbs_temp[d] = probSkew\n",
        "        #         if t == 3:\n",
        "        #             if dct[d] in keywordsConstruct4:\n",
        "        #                 wordProbs_temp[d] = probSkew\n",
        "        #         if t == 4:\n",
        "        #             if dct[d] in keywordsConstruct5:\n",
        "        #                 wordProbs_temp[d] = probSkew\n",
        "        #         if t == 5:\n",
        "        #             if dct[d] in keywordsConstruct6:\n",
        "        #                 wordProbs_temp[d] = probSkew\n",
        "        #     wordProbs.append(wordProbs_temp)\n",
        "\n",
        "        # Facility to provide asymetric prior Eta\n",
        "        wordProbs = []\n",
        "        for d in range(len(dct)):\n",
        "            wordProbs.append(10)\n",
        "            if dct[d] in keywordsConstructAll:\n",
        "                wordProbs[d] = probSkew\n",
        "\n",
        "        # Training LDA Model\n",
        "        lda_model = LdaModel(corpus=corpus,\n",
        "                                 id2word=dct,\n",
        "                                 random_state=100,\n",
        "                                 num_topics=setNoOfTopics,\n",
        "                                 passes=10,\n",
        "                                 chunksize=1000,\n",
        "                                 alpha = setAlpha,\n",
        "                                 decay=0.5,\n",
        "                                 offset=1.5,\n",
        "                                 eta = setEta,\n",
        "                                 eval_every=0,\n",
        "                                 iterations=100,\n",
        "                                 gamma_threshold=0.001,\n",
        "                                 per_word_topics=True)\n",
        "\n",
        "        # Saving LDA Model\n",
        "        lda_model.save('dataOCM/02_LDA/LDA_03_Training_Model.model')\n",
        "        lda_model.print_topics(-1)\n",
        "\n",
        "        # Reporting LDA Training to MS Excel\n",
        "        def reportIt(trainSetProps='', alphaProps='', etaProps='', trunTerms=len(dct)):\n",
        "            if tps == 0:\n",
        "                # Persisting files related to LDA Model\n",
        "                shutil.copy2('LDA_00_CorpusAnalysis.py', dir + '/LDA_00_CorpusAnalysis_' + currDateTime + '.py')\n",
        "                shutil.copy2('dataOCM/02_LDA/LDA_00_CorpusAnalysis_dctMaster.csv', dir + '/LDA_00_CorpusAnalysis_dctMaster_' + currDateTime + '.csv')\n",
        "                shutil.copy2('LDA_01_ReviewsPicker.py', dir + '/LDA_01_ReviewsPicker_' + currDateTime + '.py')\n",
        "                shutil.copy2('dataOCM/02_LDA/LDA_01_ReviewsPicker_Master_Data_for_training.csv', dir + '/LDA_01_ReviewsPicker_Master_Data_for_training_' + currDateTime + '.csv')\n",
        "                shutil.copy2('LDA_02_Preprocessing.py', dir + '/LDA_02_Preprocessing_' + currDateTime + '.py')\n",
        "                shutil.copy2('dataOCM/02_LDA/LDA_02_Preprocessing_Corpus.corpus', dir + '/LDA_02_Preprocessing_Corpus_' + currDateTime + '.corpus')\n",
        "                shutil.copy2('dataOCM/02_LDA/LDA_02_Preprocessing_Dictionary.dictionary', dir + '/LDA_02_Preprocessing_Dictionary_' + currDateTime + '.dictionary')\n",
        "            dir2 = 'dataOCM/02_LDA/LDA_Runs/01_All/' + currDateTime + '/noOfTopics_' + str(setNoOfTopics)\n",
        "            if not os.path.exists(dir2):\n",
        "                os.makedirs(dir2)\n",
        "            currDateTime2 = currDateTime + '_noOfTopics_' + str(setNoOfTopics)\n",
        "            # Persisting files related to LDA Model\n",
        "            shutil.copy2('LDA_03_Training.py', dir2 + '/LDA_03_Training_' + currDateTime2 + '.py')\n",
        "            shutil.copy2('dataOCM/02_LDA/LDA_03_Training_Model.model', dir2 + '/LDA_03_Training_Model_' + currDateTime2 + '.model')\n",
        "            shutil.copy2('dataOCM/02_LDA/LDA_03_Training_Model.model.expElogbeta.npy', dir2 + '/LDA_03_Training_Model_' + currDateTime2 + '.model.expElogbeta.npy')\n",
        "            shutil.copy2('dataOCM/02_LDA/LDA_03_Training_Model.model.id2word', dir2 + '/LDA_03_Training_Model_' + currDateTime2 + '.model.id2word')\n",
        "            shutil.copy2('dataOCM/02_LDA/LDA_03_Training_Model.model.state', dir2 + '/LDA_03_Training_Model_' + currDateTime2 + '.model.state')\n",
        "            workbook = xlsxwriter.Workbook(dir2 + '/LDA_03_Training_Report_' + currDateTime2 + '.xlsx')\n",
        "            worksheet = workbook.add_worksheet()\n",
        "\n",
        "            formatBold = workbook.add_format()\n",
        "            formatBold.set_bold()\n",
        "            formatRedLeft = workbook.add_format()\n",
        "            formatRedLeft.set_font_color('red')\n",
        "            formatRedLeft.set_align('left')\n",
        "            formatLeft = workbook.add_format()\n",
        "            formatLeft.set_align('left')\n",
        "            formatLeftBold = workbook.add_format()\n",
        "            formatLeftBold.set_bold()\n",
        "            formatLeftBold.set_align('left')\n",
        "            formatYellowLeft = workbook.add_format()\n",
        "            formatYellowLeft.set_bg_color('yellow')\n",
        "            formatYellowLeft.set_align('left')\n",
        "\n",
        "            worksheet.write(0, 0, 'Training Set Properties:', formatLeftBold)\n",
        "            worksheet.write(1, 0, trainSetProps, formatLeft)\n",
        "            worksheet.write(2, 0, 'Alpha (Reviews-Topics Probability Distribution Prior):', formatLeftBold)\n",
        "            worksheet.write(3, 0, alphaProps, formatLeft)\n",
        "            worksheet.write(4, 0, 'Eta (Terms-Topics Probability Distribution Prior):', formatLeftBold)\n",
        "            worksheet.write(5, 0, etaProps, formatLeft)\n",
        "            worksheet.write(6, 0, 'No. of Topics:', formatLeftBold)\n",
        "            worksheet.write(7, 0, setNoOfTopics, formatLeft)\n",
        "            worksheet.write(8, 0, 'Corpus length:', formatLeftBold)\n",
        "            worksheet.write(9, 0, len(corpus), formatLeft)\n",
        "            worksheet.write(10, 0, 'Dictionary Operations:', formatLeftBold)\n",
        "            dctOps = ''\n",
        "            for ops in range(len(dctOpsLog)):\n",
        "                dctOps = dctOps + str(dctOpsLog[ops]) + ' /***/ '\n",
        "            worksheet.write(11, 0, dctOps)\n",
        "            worksheet.write(12, 0, 'Topics Terms Distribution:', formatLeftBold)\n",
        "            if trunTerms == len(dct):\n",
        "                worksheet.write(13, 0, 'It is not truncated. All topics have all words of dictionary', formatLeft)\n",
        "            else:\n",
        "                worksheet.write(13, 0, 'It is truncated for top ' + str(trunTerms) + ' terms in each topic.', formatLeft)\n",
        "\n",
        "            for s in range(setNoOfTopics):\n",
        "                worksheet.write(15, s * 3, 'Topic ' + str(s), formatLeftBold)\n",
        "                worksheet.write(16, s * 3, 'Word', formatLeftBold)\n",
        "                worksheet.write(16, (s * 3) + 1, 'Probability', formatLeftBold)\n",
        "                vec = lda_model.get_topic_terms(s, trunTerms)\n",
        "                for h in range(trunTerms):\n",
        "                    if dct.id2token[vec[h][0]] in keywordsConstructAll:\n",
        "                        worksheet.write(h + 17, (s * 3), dct.id2token[vec[h][0]], formatRedLeft)\n",
        "                    else:\n",
        "                        worksheet.write(h + 17, (s * 3), dct.id2token[vec[h][0]], formatLeft)\n",
        "                    if vec[h][1] * 100 >= 5:\n",
        "                        worksheet.write(h + 17, (s * 3) + 1, vec[h][1], formatYellowLeft)\n",
        "                    else:\n",
        "                        worksheet.write(h + 17, (s * 3) + 1, vec[h][1], formatLeft)\n",
        "            workbook.close()\n",
        "\n",
        "        # Reporting function call\n",
        "        reportIt('Reviews are bundled for user and only those are considered for training which possess some word in dictionary.',\n",
        "                 'Alpha is 0.01',\n",
        "                 'Eta is 0.01',\n",
        "                 len(dct))\n",
        "\n",
        "# Takes max 20 mins"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}