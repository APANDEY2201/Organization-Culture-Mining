{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"02_LDA_00_CorpusAnalysis.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPFdO3IE7EAXtmYOYWKhXTo"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"MbSqewnS1iSV"},"source":["import nltk\n","from gensim import corpora\n","from nltk.corpus import stopwords\n","import logging\n","import csv\n","from langdetect import detect\n","import spacy\n","import xlsxwriter\n","from gensim import models\n","import os\n","import math"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mDpKECX21y-B"},"source":["# Preprocessing"]},{"cell_type":"code","metadata":{"id":"JkOOCAaa11XW"},"source":["# Whether to have nouns, verbs, adjectives in dictionary or not\n","setKeepNounInCorp = True\n","setKeepAdjInCorp = True\n","setKeepVerbInCorp = True\n","\n","# For POS tagging and lemmatization\n","nlpDe = spacy.load('de_core_news_sm')\n","nlpEn = spacy.load(\"en_core_web_sm\")\n","\n","# For lemmatization\n","def germanSpacyLemmatizer(token):\n","    token = token.lower()\n","    lemmed = ''\n","    for t in nlpDe.tokenizer(token):\n","        lemmed = lemmed + ' ' + t.lemma_\n","    return lemmed.strip()\n","def englishSpacyLemmatizer(token):\n","    token = token.lower()\n","    lemmed = ''\n","    for t in nlpEn.tokenizer(token):\n","        lemmed = lemmed + ' ' + t.lemma_\n","    return lemmed.strip()\n","\n","# For POS tagging\n","def germanSpacyPOS(token):\n","    return nlpDe(token)[0].pos_\n","def englishSpacyPOS(token):\n","    return nlpEn(token)[0].pos_\n","\n","logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s')\n","logging.root.setLevel(level=logging.INFO)\n","\n","# Stop words init\n","stop_words_en = stopwords.words('english')\n","stop_words_de = stopwords.words('german')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XeV_D4J23Iae"},"source":["# Fetching reviews and bundling for each reviewer"]},{"cell_type":"code","metadata":{"id":"N45H-Y2L2-q5"},"source":["# Input: source dir for data (corpus)\n","dir = 'dataOCM/01_MasterData_160_companies/'\n","files = [f for f in os.listdir(dir) if os.path.isfile(os.path.join(dir, f))]\n","print(files)\n","\n","# Reviews transformation as per reviewer\n","reviews = []\n","for f in range(len(files)):\n","    csvFileName = dir + files[f]\n","    masterData = list(csv.reader(open(csvFileName, encoding='utf-8'), delimiter='|'))\n","    for i in range(1, len(masterData), 10):\n","        review = masterData[i][9].strip()\n","        bigReview = ''\n","        for j in range(i, i + 10):\n","            bigReview = bigReview + ' ' + masterData[j][9].strip()\n","        reviews.append(bigReview)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JLNmHth35yB6"},"source":["# Swallowing each review to tokenize, remove stop words, lemmatize and tag POS"]},{"cell_type":"code","metadata":{"id":"ufFNMfCr5wqY"},"source":["# Swallowing each review to tokenize, remove stop words, lemmatize and tag POS\n","data_processed = []\n","tokenizer = nltk.RegexpTokenizer(r\"\\w+\")\n","listNoun = []\n","listAdj = []\n","listVerb = []\n","listNounIds = []\n","listAdjIds = []\n","listVerbIds = []\n","for doc in reviews:\n","    itsGerman = True\n","    try:\n","        if detect(doc) == 'en':\n","            itsGerman = False\n","    except:\n","        itsGerman = True\n","    doc_out = []\n","    doc = tokenizer.tokenize(doc)\n","    if itsGerman == True:\n","        for wd in doc:\n","            wd = wd.lower()\n","            if wd not in stop_words_de:\n","                lemmed_word = germanSpacyLemmatizer(wd)\n","                if (germanSpacyPOS(lemmed_word) == 'NOUN' or germanSpacyPOS(lemmed_word) == 'PROPN') and setKeepNounInCorp == True:\n","                    doc_out = doc_out + [lemmed_word]\n","                    listNoun.append(lemmed_word)\n","                if germanSpacyPOS(lemmed_word) == 'ADJ' and setKeepAdjInCorp == True:\n","                    doc_out = doc_out + [lemmed_word]\n","                    listAdj.append(lemmed_word)\n","                if germanSpacyPOS(lemmed_word) == 'VERB' and setKeepVerbInCorp == True:\n","                    doc_out = doc_out + [lemmed_word]\n","                    listVerb.append(lemmed_word)\n","            else:\n","                continue\n","    else:\n","        for wd in doc:\n","            wd = wd.lower()\n","            if wd not in stop_words_en:\n","                lemmed_word = englishSpacyLemmatizer(wd)\n","                if (englishSpacyPOS(lemmed_word) == 'NOUN' or englishSpacyPOS(lemmed_word) == 'PROPN') and setKeepNounInCorp == True:\n","                    doc_out = doc_out + [lemmed_word]\n","                    listNoun.append(lemmed_word)\n","                if englishSpacyPOS(lemmed_word) == 'ADJ' and setKeepAdjInCorp == True:\n","                    doc_out = doc_out + [lemmed_word]\n","                    listAdj.append(lemmed_word)\n","                if englishSpacyPOS(lemmed_word) == 'VERB' and setKeepVerbInCorp == True:\n","                    doc_out = doc_out + [lemmed_word]\n","                    listVerb.append(lemmed_word)\n","            else:\n","                continue\n","    data_processed.append(doc_out)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7f2X0HCY6GTX"},"source":["# Initializing the dictionary"]},{"cell_type":"code","metadata":{"id":"PrG_7K3P6JnD"},"source":["# Listing nouns, adjectives and verbs\n","listNoun = list(set(listNoun))\n","listAdj = list(set(listAdj))\n","listVerb = list(set(listVerb))\n","\n","# Initializing gensim corpora and dictionary objects\n","dct = corpora.Dictionary(data_processed)\n","corpus = [dct.doc2bow(line) for line in data_processed]\n","\n","# Populating master dictionary 2D list\n","dctMaster = {}\n","dctMaster[-1]=['Word','GlobalTF','DF','MaxTFIDF','POS','Entropy','TBD','TBD']\n","for key, value in dct.items():\n","    dctMasterTemp = []\n","    dctMasterTemp.append(value)\n","    dctMasterTemp.append(0)\n","    dctMasterTemp.append(0)\n","    dctMasterTemp.append(0)\n","    if value in listNoun:\n","        dctMasterTemp.append('NOUN') # POS\n","    elif value in listAdj:\n","        dctMasterTemp.append('ADJ') # POS\n","    elif value in listVerb:\n","        dctMasterTemp.append('VERB') # POS\n","    else:\n","        dctMasterTemp.append('UNDEFINED') # POS\n","    dctMasterTemp.append(0)\n","    dctMasterTemp.append(0)\n","    dctMasterTemp.append(0)\n","    dctMaster[key]=dctMasterTemp\n","\n","# Initializing gensim tfidf model for the corpus and dictionary\n","tfidf = models.TfidfModel(corpus, id2word=dct)\n","dctTfIDF = []"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iLxQgnND6NBT"},"source":["# Calculating different metrics related to terms in corpus"]},{"cell_type":"code","metadata":{"id":"kZQdbDn86Mmh"},"source":["# Calculating Maximum TFIDF and Global Term Frequency for each term\n","for bow in corpus:\n","    for pos in tfidf[bow]:\n","        if dctMaster[pos[0]][3]<pos[1]:\n","            dctMaster[pos[0]][3] = pos[1] # MaxTFIDF\n","    idsBow = set([id for id, qnt in bow])\n","    for id in idsBow:\n","        dctMaster[id][2] += 1 # DF\n","    for pos in bow:\n","        dctMaster[pos[0]][1] += pos[1] # GlobalTF\n","\n","# Calculating Entropy for each term in dictionary\n","for bow in corpus:\n","    for pos in bow:\n","        dctMaster[pos[0]][5] += -(pos[1]/dctMaster[pos[0]][1])*math.log(pos[1]/dctMaster[pos[0]][1],2) # Entropy\n","oneByLog2D = 1/math.log(len(corpus),2)\n","for key in dctMaster:\n","    if key!=-1:\n","        dctMaster[key][5] *= oneByLog2D\n","\n","# Output: entire dictionary along with calculated metrics MS Excel file\n","workbook = xlsxwriter.Workbook('dataOCM/02_LDA/LDA_00_CorpusAnalysis_dctMaster.xlsx')\n","worksheet = workbook.add_worksheet()\n","cnt1 = 0\n","for key in dctMaster:\n","    cnt2 = 0\n","    for val in dctMaster[key]:\n","        worksheet.write(cnt1,cnt2,val)\n","        cnt2 += 1\n","    cnt1 += 1\n","workbook.close()\n","\n","# Takes approximately 5 hours to process the kununu reviews of all 160 companies on Dell Precision, 16GB RAM, intel core i7"],"execution_count":null,"outputs":[]}]}