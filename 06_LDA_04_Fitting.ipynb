{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "06_LDA_04_Fitting.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "WqAfNekdHLX4"
      },
      "source": [
        "import nltk\n",
        "from gensim.models import LdaModel, LdaMulticore\n",
        "from gensim import corpora\n",
        "from nltk.corpus import stopwords\n",
        "import csv\n",
        "from langdetect import detect\n",
        "import spacy\n",
        "import xlsxwriter\n",
        "import shutil\n",
        "import os\n",
        "import pickle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SrG7RE_PHik6"
      },
      "source": [
        "# Manual Intervention\n",
        "In dataOCM/02_LDA/<span></span>LDA_Runs/01_All, there are models trained on different number of topics\n",
        "\n",
        "Copy the timestamp folder belonging to LDA Model for dataset1 and dataset2 to dataOCM/02_LDA/<span></span>LDA_Runs/03_Selected\n",
        "\n",
        "Supply the folder name and no. of topics in below mentioned if condition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H_BEOdoMH9Vl"
      },
      "source": [
        "# Dataset 1/2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jSM2zV0BH3cA"
      },
      "source": [
        "# Choose the dataset (1 for culture dataset, 2 for diversity dataset)\n",
        "dataset = 1\n",
        "\n",
        "# Dataset switch\n",
        "if dataset == 1:\n",
        "    LDAModelTimeStamp = '07092020_113759' # Dataset1\n",
        "    noOfTopicsFolder = 'noOfTopics_18'\n",
        "elif dataset == 2:\n",
        "    LDAModelTimeStamp = '08092020_083631'  # Dataset2\n",
        "    noOfTopicsFolder = 'noOfTopics_23'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ifoTAjIdIDwG"
      },
      "source": [
        "# LDA Model target location"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BWDmGoDXILA-"
      },
      "source": [
        "dir = 'dataOCM/02_LDA/LDA_Runs/03_Selected/' + LDAModelTimeStamp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NtsRM6aUIUZc"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MAjrBSwGIW0O"
      },
      "source": [
        "setFitBundle = False\n",
        "setTempRun = ''\n",
        "\n",
        "# For lemmatization and POS tagging\n",
        "nlpDe = spacy.load('de_core_news_sm')\n",
        "nlpEn = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Regex tokenization\n",
        "tokenizer = nltk.RegexpTokenizer(r\"\\w+\")\n",
        "\n",
        "# For stop words\n",
        "stop_words_en = stopwords.words('english')\n",
        "stop_words_de = stopwords.words('german')\n",
        "\n",
        "# For lemmatization\n",
        "def germanSpacyLemmatizer(token):\n",
        "    token = token.lower()\n",
        "    lemmed = ''\n",
        "    for t in nlpDe.tokenizer(token):\n",
        "        lemmed = lemmed + ' ' + t.lemma_\n",
        "    return lemmed.strip()\n",
        "def englishSpacyLemmatizer(token):\n",
        "    token = token.lower()\n",
        "    lemmed = ''\n",
        "    for t in nlpEn.tokenizer(token):\n",
        "        lemmed = lemmed + ' ' + t.lemma_\n",
        "    return lemmed.strip()\n",
        "\n",
        "# For POS tagging\n",
        "def germanSpacyPOS(token):\n",
        "    return nlpDe(token)[0].pos_\n",
        "def englishSpacyPOS(token):\n",
        "    return nlpEn(token)[0].pos_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KBleIbG4IbkQ"
      },
      "source": [
        "# Load the LDA model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uO9TFoj1Id9W"
      },
      "source": [
        "# Load the LDA model\n",
        "lda_model = LdaModel.load(dir + '/' + noOfTopicsFolder + '/' + 'LDA_03_Training_Model_' + LDAModelTimeStamp + '_' + noOfTopicsFolder + '.model')\n",
        "noOfTopics = lda_model.num_topics\n",
        "dct = corpora.dictionary.Dictionary.load(dir + '/' + 'LDA_02_Preprocessing_Dictionary_' + LDAModelTimeStamp + '.dictionary')\n",
        "lda_model.print_topics(-1)\n",
        "\n",
        "benchmarkReviews = []\n",
        "fittedData = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9CPJwkoUIjoU"
      },
      "source": [
        "# Input: data for training LDA model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jwdx3vEDIjFs"
      },
      "source": [
        "csvFileName = dir + '/' + 'LDA_01_ReviewsPicker_Master_Data_for_training_' + LDAModelTimeStamp + '.csv'\n",
        "masterDataBig = list(csv.reader(open(csvFileName, encoding='utf-8'), delimiter='|'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N0ZuYoACImDF"
      },
      "source": [
        "# Output: data fitted by LDA Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VpLwAr2fIoGd"
      },
      "source": [
        "csvFileNameOut = dir + '/' + noOfTopicsFolder + '/' + 'LDA_04_Fitting_Master_Data_fitted_' + LDAModelTimeStamp + '.csv'\n",
        "csvFileOut = open(csvFileNameOut, \"w\", newline='', encoding='utf-8')\n",
        "csv_out = csv.writer(csvFileOut, delimiter='|')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i-7HFX8HIqKl"
      },
      "source": [
        "# Starting to write in csv as well as in list"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z8RQNRGnIs6S"
      },
      "source": [
        "csv_out.writerow(masterDataBig[0] + ['topic' + str(i) for i in range(noOfTopics)])\n",
        "fittedData.append(masterDataBig[0] + ['topic' + str(i) for i in range(noOfTopics)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nIintH8AIxqE"
      },
      "source": [
        "# Looping through input reviews data and fitting each review with topic proportions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WBK5yefuIymV"
      },
      "source": [
        "loopStep = 1\n",
        "if setFitBundle == True:\n",
        "    loopStep = 10\n",
        "for j in range(1, len(masterDataBig),loopStep):\n",
        "    doc = masterDataBig[j][9].strip()\n",
        "    if setFitBundle == True:\n",
        "        doc = ''\n",
        "        for k in range(j, j + 10):\n",
        "            doc = doc + ' ' + masterDataBig[k][9].strip()\n",
        "        masterDataBig[j][9] = doc\n",
        "    if len(doc) > 5:\n",
        "        itsGerman = True\n",
        "        try:\n",
        "            if detect(doc) == 'en':\n",
        "                itsGerman = False\n",
        "        except:\n",
        "            itsGerman = True\n",
        "        doc_out = []\n",
        "        doc = tokenizer.tokenize(doc)\n",
        "        if itsGerman == True:\n",
        "            for wd in doc:\n",
        "                wd = wd.lower()\n",
        "                if wd not in stop_words_de:\n",
        "                    lemmed_word = germanSpacyLemmatizer(wd)\n",
        "                    if lemmed_word:\n",
        "                        doc_out = doc_out + [lemmed_word]\n",
        "                else:\n",
        "                    continue\n",
        "        else:\n",
        "            for wd in doc:\n",
        "                wd = wd.lower()\n",
        "                if wd not in stop_words_en:\n",
        "                    lemmed_word = englishSpacyLemmatizer(wd)\n",
        "                    if lemmed_word:\n",
        "                        doc_out = doc_out + [lemmed_word]\n",
        "                else:\n",
        "                    continue\n",
        "        corpus2 = [dct.doc2bow(doc_out)]\n",
        "        vector = lda_model[corpus2[0]]\n",
        "        vector2 = vector[0]\n",
        "        finalVector = []\n",
        "        for k in range(noOfTopics):\n",
        "            finalVector_temp = []\n",
        "            finalVector_temp.append(k)\n",
        "            finalVector_temp.append(0)\n",
        "            for l in range(len(vector2)):\n",
        "                if vector2[l][0] == k:\n",
        "                    finalVector_temp[1] = vector2[l][1]\n",
        "            finalVector.append(finalVector_temp)\n",
        "        csv_out.writerow(masterDataBig[j] + [row[1] for row in finalVector])\n",
        "        fittedData.append(masterDataBig[j] + [row[1] for row in finalVector])\n",
        "        if len(masterDataBig[j][9].strip()) > 1:\n",
        "            benchmarkReviewsTemp = []\n",
        "            benchmarkReviewsTemp.append(masterDataBig[j][7].strip())\n",
        "            benchmarkReviewsTemp.append(masterDataBig[j][8].strip())\n",
        "            benchmarkReviewsTemp.append(masterDataBig[j][9].strip())\n",
        "            benchmarkReviewsTemp.append([row[1] for row in finalVector])\n",
        "            benchmarkReviews.append(benchmarkReviewsTemp)\n",
        "    if j % 100 == 0:\n",
        "        print(str(j) + \" reviews processed.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IRdkIWqPI2qU"
      },
      "source": [
        "# Reporting LDA Fitting to MS Excel"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BGeoRFQ1I4uY"
      },
      "source": [
        "def reportIt():\n",
        "    # Fixing target folder\n",
        "    if not os.path.exists(dir):\n",
        "        os.makedirs(dir)\n",
        "    #shutil.copy2('LDA_04_Fitting.py', dir + '/' + noOfTopicsFolder + '/' +  'LDA_04_Fitting_' + LDAModelTimeStamp + '.py')\n",
        "    with open(dir + '/' + noOfTopicsFolder + '/' +  'LDA_04_Fitting_fittedData_' + LDAModelTimeStamp + '.data', 'wb') as f:\n",
        "        pickle.dump(fittedData, f)\n",
        "    workbook = xlsxwriter.Workbook(dir + '/' + noOfTopicsFolder + '/' + 'LDA_04_Fitting_Report_' + LDAModelTimeStamp + '.xlsx')\n",
        "    worksheet2 = workbook.add_worksheet()\n",
        "\n",
        "    formatBold = workbook.add_format()\n",
        "    formatBold.set_bold()\n",
        "    formatRedLeft = workbook.add_format()\n",
        "    formatRedLeft.set_font_color('red')\n",
        "    formatRedLeft.set_align('left')\n",
        "    formatLeft = workbook.add_format()\n",
        "    formatLeft.set_align('left')\n",
        "    formatLeftBold = workbook.add_format()\n",
        "    formatLeftBold.set_bold()\n",
        "    formatLeftBold.set_align('left')\n",
        "\n",
        "    worksheet2.write(0, 0, 'Applying the model on some Benchmark Reviews:', formatLeftBold)\n",
        "    worksheet2.write(2, 0, 'ReviewAbout', formatLeftBold)\n",
        "    worksheet2.write(2, 1, 'ReviewScore', formatLeftBold)\n",
        "    worksheet2.write(2, 2, 'Review', formatLeftBold)\n",
        "    for s in range(noOfTopics):\n",
        "        worksheet2.write(2, s + 3, 'Topic ' + str(s), formatLeftBold)\n",
        "    worksheet2.write(2, noOfTopics + 3, 'Peak Topic', formatLeftBold)\n",
        "    for s in range (len(benchmarkReviews)):\n",
        "        worksheet2.write(s + 3, 0, str(benchmarkReviews[s][0]), formatLeft)\n",
        "        worksheet2.write(s + 3, 1, str(benchmarkReviews[s][1]), formatLeft)\n",
        "        worksheet2.write(s + 3, 2, str(benchmarkReviews[s][2]), formatLeft)\n",
        "        for o in range(noOfTopics):\n",
        "            worksheet2.write(s + 3, o + 3, benchmarkReviews[s][3][o], formatLeft)\n",
        "        worksheet2.write(s+3, noOfTopics + 3, benchmarkReviews[s][3].index(max(benchmarkReviews[s][3])), formatLeft)\n",
        "    workbook.close()\n",
        "\n",
        "# Reporting function call\n",
        "reportIt()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}