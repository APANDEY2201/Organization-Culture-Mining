{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "03_LDA_02_Preprocessing.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "59YcpYqN9TpD"
      },
      "source": [
        "import csv\n",
        "import os\n",
        "import sys\n",
        "from langdetect import detect\n",
        "import spacy\n",
        "import nltk\n",
        "from nltk.corpus import stopwords"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_FD8238u9ebo"
      },
      "source": [
        "# Dataset 1/2 and calling seed words file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h1cOETO99lxE"
      },
      "source": [
        "# Choose the dataset (1 for culture dataset, 2 for diversity dataset)\n",
        "dataset = 1\n",
        "\n",
        "# Input: seed words\n",
        "csvFileName = 'dataOCM/02_LDA/LDA_00_CorpusAnalysis_KeywordsTable_output_IMP.csv'\n",
        "keywordsTable = list(csv.reader(open(csvFileName,encoding='utf-8'),delimiter=','))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ou44fss69xEs"
      },
      "source": [
        "# Manual Intervention\n",
        "From the MS Excel file dataOCM/02_LDA<span></span>/LDA_00_CorpusAnalysis_dctMaster.xlsx...\n",
        "\n",
        "Select the words to be needed in the dictionary...\n",
        "\n",
        "And export to csv at dataOCM/02_LDA<span></span>/LDA_00_CorpusAnalysis_dctMaster.csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VT1hzrPS_XyP"
      },
      "source": [
        "# Fetching domain dictionary and filtering it as per dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EyrMZfGp92_H"
      },
      "source": [
        "# Input: entire dictionary along with calculated metrics MS Excel file (Processed manually)\n",
        "csvFileName = 'dataOCM/02_LDA/LDA_00_CorpusAnalysis_dctMaster.csv'\n",
        "dctMaster = list(csv.reader(open(csvFileName,encoding='utf-8'),delimiter=','))\n",
        "if dataset == 1:\n",
        "    dctWords = [dctMaster[i][0] for i in range(1,len(dctMaster))] # Dataset1\n",
        "elif dataset == 2:\n",
        "    dctWords = [dctMaster[i][0] for i in range(1,len(dctMaster)) if len(dctMaster[i][6]) == 2] # Dataset2\n",
        "else:\n",
        "    print(\"dataset must be 1 or 2!\")\n",
        "    sys.exit()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k-lHRGgs_e0f"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TktqYV57_kPo"
      },
      "source": [
        "# For lemmatization and POS tagging\n",
        "nlpDe = spacy.load('de_core_news_sm')\n",
        "nlpEn = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# For stop words\n",
        "stop_words_en = stopwords.words('english')\n",
        "stop_words_de = stopwords.words('german')\n",
        "\n",
        "# Regex tokenization\n",
        "tokenizer = nltk.RegexpTokenizer(r\"\\w+\")\n",
        "\n",
        "# Unique list\n",
        "def unique(list1):\n",
        "    unique_list = []\n",
        "    for x in list1:\n",
        "        if x not in unique_list:\n",
        "            unique_list.append(x)\n",
        "    return unique_list\n",
        "\n",
        "# For lemmatization\n",
        "def germanSpacyLemmatizer(token):\n",
        "    token = token.lower()\n",
        "    lemmed = ''\n",
        "    for t in nlpDe.tokenizer(token):\n",
        "        lemmed = lemmed + ' ' + t.lemma_\n",
        "    return lemmed.strip()\n",
        "def englishSpacyLemmatizer(token):\n",
        "    token = token.lower()\n",
        "    lemmed = ''\n",
        "    for t in nlpEn.tokenizer(token):\n",
        "        lemmed = lemmed + ' ' + t.lemma_\n",
        "    return lemmed.strip()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PRGodI6-_lJ3"
      },
      "source": [
        "# Exporting seed words in flat format to csv files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BrHrz9Pl_opb"
      },
      "source": [
        "keywordsTableDe = []\n",
        "keywordsTableEn = []\n",
        "keywordsOutDe = []\n",
        "keywordsOutEn = []\n",
        "for j in range(len(keywordsTable)):\n",
        "    for k in range(1,len(keywordsTable[j])):\n",
        "        keyword = keywordsTable[j][k]\n",
        "        keywordLang = keyword[0:3]\n",
        "        keyword = keyword.replace('en:','').replace('de:','')\n",
        "        itsGermanKeyword = True\n",
        "        if keywordLang == \"en:\":\n",
        "            itsGermanKeyword = False\n",
        "        else:\n",
        "            itsGermanKeyword = True\n",
        "        if itsGermanKeyword == True:\n",
        "            keyword = germanSpacyLemmatizer(keyword)\n",
        "            keywordsTableDe.append(keyword)\n",
        "            keywordsOutDe_temp = []\n",
        "            keywordsOutDe_temp.append(keywordsTable[j][0])\n",
        "            keywordsOutDe_temp.append(keyword)\n",
        "            keywordsOutDe.append(keywordsOutDe_temp)\n",
        "        else:\n",
        "            keyword = englishSpacyLemmatizer(keyword)\n",
        "            keywordsTableEn.append(keyword)\n",
        "            keywordsOutEn_temp = []\n",
        "            keywordsOutEn_temp.append(keywordsTable[j][0])\n",
        "            keywordsOutEn_temp.append(keyword)\n",
        "            keywordsOutEn.append(keywordsOutEn_temp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0SjilEbu_rih"
      },
      "source": [
        "# Output: seed words flattened csv"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RXDRIS3-_trN"
      },
      "source": [
        "keywordsTableEn = unique(keywordsTableEn)\n",
        "keywordsTableDe = unique(keywordsTableDe)\n",
        "csvFileNameOut = 'dataOCM/02_LDA/LDA_01_ReviewsPicker_keywordsEn.csv'\n",
        "csvFileOut = open(csvFileNameOut, \"w\", newline='', encoding='utf-8')\n",
        "csv_out = csv.writer(csvFileOut, delimiter=',')\n",
        "for c in range(len(keywordsOutEn)):\n",
        "    csv_out.writerow(keywordsOutEn[c]) # + features)\n",
        "csvFileNameOut = 'dataOCM/02_LDA/LDA_01_ReviewsPicker_keywordsDe.csv'\n",
        "csvFileOut = open(csvFileNameOut, \"w\", newline='', encoding='utf-8')\n",
        "csv_out = csv.writer(csvFileOut, delimiter=',')\n",
        "for c in range(len(keywordsOutDe)):\n",
        "    csv_out.writerow(keywordsOutDe[c]) # + features)\n",
        "print('Keywords files created.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aNwCaf6P_xqo"
      },
      "source": [
        "# Checking if review has dictionary word or not"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LiwJbGI2_xK9"
      },
      "source": [
        "def reviewHit(review):\n",
        "    fetchThis = False\n",
        "    doc = review\n",
        "    itsGerman = True\n",
        "    try:\n",
        "        if detect(doc) == 'en':\n",
        "            itsGerman = False\n",
        "    except:\n",
        "        itsGerman = True\n",
        "    doc = tokenizer.tokenize(doc)\n",
        "    if itsGerman == True:\n",
        "        for wd in doc:\n",
        "            wd = wd.lower()\n",
        "            if wd not in stop_words_de:\n",
        "                lemmed_word = germanSpacyLemmatizer(wd)\n",
        "                if lemmed_word in dctWords:\n",
        "                    fetchThis = True\n",
        "                    return fetchThis\n",
        "            else:\n",
        "                continue\n",
        "    else:\n",
        "        for wd in doc:\n",
        "            wd = wd.lower()\n",
        "            if wd not in stop_words_en:\n",
        "                lemmed_word = englishSpacyLemmatizer(wd)\n",
        "                if lemmed_word in dctWords:\n",
        "                    fetchThis = True\n",
        "                    return fetchThis\n",
        "            else:\n",
        "                continue"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T--zF31R_3zW"
      },
      "source": [
        "# Counting number of words in a review"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KecL1jb7_59O"
      },
      "source": [
        "def wordsCounter(review):\n",
        "    doc = review\n",
        "    itsGerman = True\n",
        "    try:\n",
        "        if detect(doc) == 'en':\n",
        "            itsGerman = False\n",
        "    except:\n",
        "        itsGerman = True\n",
        "    doc = tokenizer.tokenize(doc)\n",
        "    wordsCount = 0\n",
        "    if itsGerman == True:\n",
        "        for wd in doc:\n",
        "            wd = wd.lower()\n",
        "            if wd not in stop_words_de:\n",
        "                wordsCount = wordsCount + 1\n",
        "            else:\n",
        "                continue\n",
        "    else:\n",
        "        for wd in doc:\n",
        "            wd = wd.lower()\n",
        "            if wd not in stop_words_en:\n",
        "                wordsCount = wordsCount + 1\n",
        "            else:\n",
        "                continue\n",
        "    return wordsCount"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Or1E81Wq_80l"
      },
      "source": [
        "# Output: data for training LDA model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FMbG7qxJABzY"
      },
      "source": [
        "csvFileNameOut = 'dataOCM/02_LDA/LDA_01_ReviewsPicker_Master_Data_for_training.csv'\n",
        "csvFileOut = open(csvFileNameOut, \"w\", newline='', encoding='utf-8')\n",
        "csv_out = csv.writer(csvFileOut, delimiter='|')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MnjlltpUADja"
      },
      "source": [
        "# Input: source dir for data (corpus)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YiaO5ThxAFn8"
      },
      "source": [
        "dir = 'dataOCM/01_MasterData_160_companies/'\n",
        "files = [f for f in os.listdir(dir) if os.path.isfile(os.path.join(dir, f))]\n",
        "print(files)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i4k7QI2UAKKr"
      },
      "source": [
        "# Processing each review from corpus to decide to select it or not for training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G0WfPzxiALAt"
      },
      "source": [
        "firstFile = True\n",
        "totalReviews = 0\n",
        "for f in range(len(files)):\n",
        "    csvFileName = dir + files[f]\n",
        "    masterData = list(csv.reader(open(csvFileName, encoding='utf-8'), delimiter='|'))  # CSV file to 2 dimensional list of string\n",
        "\n",
        "    if firstFile:\n",
        "        csv_out.writerow(masterData[0] + ['RvScoreWorkAtmosphere','RvScoreCohesionAmongColleagues','RvScoreEqualRights','RvScoreDealingWithOlderColleagues','RvScoreEnvironmentalSocialAwareness','Corona1','Corona2','Corona3','RvScoreWorkLifeBalance', 'noOfWords']) # + features)\n",
        "        # Corona1: Wofür möchtest du deinen Arbeitgeber im Umgang mit der Corona-Situation loben?\n",
        "        # Corona2: Was macht dein Arbeitgeber im Umgang mit der Corona-Situation nicht gut? / Wo siehst du Chancen für deinen Arbeitgeber mit der Corona-Situation besser umzugehen?\n",
        "        # Corona3: Wie kann dich dein Arbeitgeber im Umgang mit der Corona-Situation noch besser unterstützen?\n",
        "        firstFile = False\n",
        "\n",
        "    # for i in range(1,len(masterData)):\n",
        "    #     review = masterData[i][9].strip()\n",
        "    #     # if (review != ''):\n",
        "    #     #     csv_out.writerow(masterData[i])\n",
        "    #     # if ((masterData[i][7] == 'Gleichberechtigung' or masterData[i][7] ==  'Umgang mit älteren Kollegen') and review != '') or reviewHit(review) == True:\n",
        "    #     #     csv_out.writerow(masterData[i])\n",
        "    #     if reviewHit(review) == True and review != '':\n",
        "    #         csv_out.writerow(masterData[i])\n",
        "    #\n",
        "    #     if i%100 == 0:\n",
        "    #         print(str(i) + \" reviews processed.\")\n",
        "\n",
        "    for i in range(1,len(masterData),10):\n",
        "        review = masterData[i][9].strip()\n",
        "        bigReview = ''\n",
        "        ratingsList = []\n",
        "        for j in range(i,i+10):\n",
        "            bigReview = bigReview + ' ' + masterData[j][9].strip()\n",
        "            if j!=i:\n",
        "                ratingsList.append(masterData[j][8].strip())\n",
        "        masterData[i][9] = bigReview\n",
        "        masterData[i].extend(ratingsList)\n",
        "        if (len(bigReview.strip()) > 50 and reviewHit(bigReview) == True):\n",
        "            csv_out.writerow(masterData[i]+[wordsCounter(bigReview)])\n",
        "            totalReviews += 1\n",
        "        # if ((masterData[i][7] == 'Gleichberechtigung' or masterData[i][7] ==  'Umgang mit älteren Kollegen') and review != '') or reviewHit(review) == True:\n",
        "        #     csv_out.writerow(masterData[i])\n",
        "        # if reviewHit(review) == True:\n",
        "        #     csv_out.writerow(masterData[i])\n",
        "\n",
        "        if i%100 == 0:\n",
        "            print(str(i) + \" reviews processed.\")\n",
        "\n",
        "print('total reviews are', str(totalReviews))\n",
        "\n",
        "# In case there is need to check picked reviews\n",
        "\n",
        "# csvFileNameOut = 'pickedReviews.csv'\n",
        "# csvFileOut = open(csvFileNameOut, \"w\", newline='', encoding='utf-8')\n",
        "# csv_out = csv.writer(csvFileOut, delimiter='|')\n",
        "# csv_out.writerow(masterData[0][7:10]) # + features)\n",
        "\n",
        "# takes approx half hour to process 160 companies reviews"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
