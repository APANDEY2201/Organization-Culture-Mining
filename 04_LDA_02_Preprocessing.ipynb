{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "04_LDA_02_Preprocessing.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ERMQVLonBVob"
      },
      "source": [
        "import nltk\n",
        "from gensim import corpora\n",
        "from nltk.corpus import stopwords\n",
        "import logging\n",
        "import csv\n",
        "from langdetect import detect\n",
        "import spacy\n",
        "import pickle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_gLiH5NHCjtt"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MGjcS7UHChQB"
      },
      "source": [
        "# Whether to have nouns, verbs, adjectives in dictionary or not\n",
        "setKeepNounInCorp = True\n",
        "setKeepAdjInCorp = True\n",
        "setKeepVerbInCorp = True\n",
        "\n",
        "# Whether to run temporarily for troubleshooting\n",
        "setTempRun = ''\n",
        "\n",
        "# For POS tagging and lemmatization\n",
        "nlpDe = spacy.load('de_core_news_sm')\n",
        "nlpEn = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Check if the word is duplicate to another in dictionary\n",
        "duplicateDictTermsDict = {'arbeitszeiten':'arbeitszeit','arbeiten':'arbeit','nette':'nett','mitarbeitern':'mitarbeiter','interessante':'interessant','teams':'team','neue':'neu','gutes':'gut','abteilungen':'abteilung'}\n",
        "def duplicateDictTerms(term):\n",
        "    if term in duplicateDictTermsDict:\n",
        "        return duplicateDictTermsDict[term]\n",
        "    else:\n",
        "        return term\n",
        "\n",
        "# For lemmatization\n",
        "def germanSpacyLemmatizer(token):\n",
        "    token = token.lower()\n",
        "    lemmed = ''\n",
        "    for t in nlpDe.tokenizer(token):\n",
        "        lemmed = lemmed + ' ' + t.lemma_\n",
        "    term = duplicateDictTerms(lemmed.strip())\n",
        "    return term\n",
        "\n",
        "# For lemmatization\n",
        "def englishSpacyLemmatizer(token):\n",
        "    token = token.lower()\n",
        "    lemmed = ''\n",
        "    for t in nlpEn.tokenizer(token):\n",
        "        lemmed = lemmed + ' ' + t.lemma_\n",
        "    term = duplicateDictTerms(lemmed.strip())\n",
        "    return term\n",
        "\n",
        "# For POS tagging\n",
        "def germanSpacyPOS(token):\n",
        "    return nlpDe(token)[0].pos_\n",
        "def englishSpacyPOS(token):\n",
        "    return nlpEn(token)[0].pos_\n",
        "\n",
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s')\n",
        "logging.root.setLevel(level=logging.INFO)\n",
        "\n",
        "# Stop words init\n",
        "stop_words_en = stopwords.words('english')\n",
        "stop_words_de = stopwords.words('german')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1LrPrtaHCpeX"
      },
      "source": [
        "# Input: data for training LDA model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "46tBTxqgCobf"
      },
      "source": [
        "# Input: data for training LDA model\n",
        "csvFileName1 = 'dataOCM/02_LDA/LDA_01_ReviewsPicker_Master_Data_for_training' + setTempRun + '.csv'\n",
        "masterDataSmall = list(csv.reader(open(csvFileName1, encoding='utf-8'), delimiter='|'))\n",
        "reviews = [masterDataSmall[row][9] for row in range(1,len(masterDataSmall))]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kGJIIbCwCwMZ"
      },
      "source": [
        "# Swallowing each review to tokenize, remove stop words, lemmatize and tag POS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MBLIP3ZWCqQw"
      },
      "source": [
        "# Swallowing each review to tokenize, remove stop words, lemmatize and tag POS\n",
        "data_processed = []\n",
        "tokenizer = nltk.RegexpTokenizer(r\"\\w+\")\n",
        "listNoun = []\n",
        "listAdj = []\n",
        "listVerb = []\n",
        "listNounIds = []\n",
        "listAdjIds = []\n",
        "listVerbIds = []\n",
        "for doc in reviews:\n",
        "    itsGerman = True\n",
        "    try:\n",
        "        if detect(doc) == 'en':\n",
        "            itsGerman = False\n",
        "    except:\n",
        "        itsGerman = True\n",
        "    doc_out = []\n",
        "    doc = tokenizer.tokenize(doc)\n",
        "    if itsGerman == True:\n",
        "        for wd in doc:\n",
        "            wd = wd.lower()\n",
        "            if wd not in stop_words_de:\n",
        "                lemmed_word = germanSpacyLemmatizer(wd)\n",
        "                if (germanSpacyPOS(lemmed_word) == 'NOUN' or germanSpacyPOS(lemmed_word) == 'PROPN') and setKeepNounInCorp == True:\n",
        "                    doc_out = doc_out + [lemmed_word]\n",
        "                    listNoun.append(lemmed_word)\n",
        "                if germanSpacyPOS(lemmed_word) == 'ADJ' and setKeepAdjInCorp == True:\n",
        "                    doc_out = doc_out + [lemmed_word]\n",
        "                    listAdj.append(lemmed_word)\n",
        "                if germanSpacyPOS(lemmed_word) == 'VERB' and setKeepVerbInCorp == True:\n",
        "                    doc_out = doc_out + [lemmed_word]\n",
        "                    listVerb.append(lemmed_word)\n",
        "            else:\n",
        "                continue\n",
        "    else:\n",
        "        for wd in doc:\n",
        "            wd = wd.lower()\n",
        "            if wd not in stop_words_en:\n",
        "                lemmed_word = englishSpacyLemmatizer(wd)\n",
        "                if (englishSpacyPOS(lemmed_word) == 'NOUN' or englishSpacyPOS(lemmed_word) == 'PROPN') and setKeepNounInCorp == True:\n",
        "                    doc_out = doc_out + [lemmed_word]\n",
        "                    listNoun.append(lemmed_word)\n",
        "                if englishSpacyPOS(lemmed_word) == 'ADJ' and setKeepAdjInCorp == True:\n",
        "                    doc_out = doc_out + [lemmed_word]\n",
        "                    listAdj.append(lemmed_word)\n",
        "                if englishSpacyPOS(lemmed_word) == 'VERB' and setKeepVerbInCorp == True:\n",
        "                    doc_out = doc_out + [lemmed_word]\n",
        "                    listVerb.append(lemmed_word)\n",
        "            else:\n",
        "                continue\n",
        "    data_processed.append(doc_out)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b2rb7jM3CyCN"
      },
      "source": [
        "# Forming the dictionary and POS lists"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PSQoTFwIDLy6"
      },
      "source": [
        "# Listing nouns, adjectives and verbs\n",
        "listNoun = list(set(listNoun))\n",
        "listAdj = list(set(listAdj))\n",
        "listVerb = list(set(listVerb))\n",
        "\n",
        "# Initializing gensim corpora and dictionary objects\n",
        "dct = corpora.Dictionary(data_processed)\n",
        "corpus = [dct.doc2bow(line) for line in data_processed]\n",
        "\n",
        "# Segregating the seed words as per their groups\n",
        "wordProbs = []\n",
        "csvFileName1 = 'dataOCM/02_LDA/LDA_01_ReviewsPicker_keywordsDe.csv'\n",
        "impKeywordsDe = list(csv.reader(open(csvFileName1, encoding='utf-8'), delimiter=','))\n",
        "impKeywordsDeFinal = [i[0] for i in impKeywordsDe]\n",
        "csvFileName1 = 'dataOCM/02_LDA/LDA_01_ReviewsPicker_keywordsEn.csv'\n",
        "impKeywordsEn = list(csv.reader(open(csvFileName1, encoding='utf-8'), delimiter=','))\n",
        "impKeywordsEnFinal = [i[0] for i in impKeywordsEn]\n",
        "keywordsConstruct1 = [row[1] for row in [row for row in impKeywordsDe+impKeywordsEn if 'overall' == row[0]]]\n",
        "keywordsConstruct2 = [row[1] for row in [row for row in impKeywordsDe+impKeywordsEn if 'gender' == row[0]]]\n",
        "keywordsConstruct3 = [row[1] for row in [row for row in impKeywordsDe+impKeywordsEn if 'age' == row[0]]]\n",
        "keywordsConstruct4 = [row[1] for row in [row for row in impKeywordsDe+impKeywordsEn if 'cultural background' == row[0]]]\n",
        "keywordsConstruct5 = [row[1] for row in [row for row in impKeywordsDe+impKeywordsEn if 'sexual orientation' == row[0]]]\n",
        "keywordsConstruct6 = [row[1] for row in [row for row in impKeywordsDe+impKeywordsEn if 'handicap' == row[0]]]\n",
        "keywordsConstructAll = keywordsConstruct1+keywordsConstruct2+keywordsConstruct3+keywordsConstruct4+keywordsConstruct5+keywordsConstruct6\n",
        "keywordsConstructAllIDsInDct = []\n",
        "\n",
        "# Input: entire dictionary along with calculated metrics MS Excel file (Processed manually)\n",
        "csvFileName = 'dataOCM/02_LDA/LDA_00_CorpusAnalysis_dctMaster.csv'\n",
        "dctMaster = list(csv.reader(open(csvFileName, encoding='utf-8'), delimiter=','))\n",
        "dctWords = [dctMaster[sa][0] for sa in range(1,len(dctMaster))]\n",
        "dctWordsIds = []\n",
        "\n",
        "keywordsConstructAllNew = []\n",
        "keywordsConstructAllIDsInDctNew = []\n",
        "listNounNew = []\n",
        "listAdjNew = []\n",
        "listVerbNew = []\n",
        "listNounIdsNew = []\n",
        "listAdjIdsNew = []\n",
        "listVerbIdsNew = []\n",
        "\n",
        "# Populating nouns, verbs and adjectives lists IDS\n",
        "for token, id in dct.token2id.items():\n",
        "    if token in keywordsConstructAll:\n",
        "        keywordsConstructAllIDsInDct.append(id)\n",
        "    if token in listNoun:\n",
        "        listNounIds.append(id)\n",
        "    if token in listAdj:\n",
        "        listAdjIds.append(id)\n",
        "    if token in listVerb:\n",
        "        listVerbIds.append(id)\n",
        "    if token in dctWords:\n",
        "        dctWordsIds.append(id)\n",
        "\n",
        "# Log generation\n",
        "dctOpsLog = []\n",
        "dctOpsLog.append('Dictionary contains ' + str(len(dct)) + ' terms (Nouns: ' + str(len(listNounIds)) + ' / Adjs: ' + str(len(listAdjIds)) + ' / Verbs: ' + str(len(listVerbIds)) + ' / ImpKeywords: ' + str(len(keywordsConstructAllIDsInDct)) + ') before filtering out bad terms.')\n",
        "print(dctOpsLog[-1])\n",
        "dctOpsLog.append('Filtering the dictionary to keep only the important terms...')\n",
        "print(dctOpsLog[-1])\n",
        "\n",
        "# Dictionary filtration\n",
        "dct.filter_tokens(good_ids=list(dctWordsIds))\n",
        "finalNosImpKeywords = 0\n",
        "finalNosNouns = 0\n",
        "finalNosAdjs = 0\n",
        "finalNosVerbs = 0\n",
        "\n",
        "# Populating nouns, verbs and adjectives lists words\n",
        "for token, id in dct.token2id.items():\n",
        "    if token in keywordsConstructAll:\n",
        "        finalNosImpKeywords = finalNosImpKeywords + 1\n",
        "        keywordsConstructAllNew.append(token)\n",
        "        keywordsConstructAllIDsInDctNew.append((id))\n",
        "    if token in listNoun:\n",
        "        finalNosNouns = finalNosNouns + 1\n",
        "        listNounNew.append(token)\n",
        "        listNounIdsNew.append(id)\n",
        "    if token in listAdj:\n",
        "        finalNosAdjs = finalNosAdjs + 1\n",
        "        listAdjNew.append(token)\n",
        "        listAdjIdsNew.append(id)\n",
        "    if token in listVerb:\n",
        "        finalNosVerbs = finalNosVerbs + 1\n",
        "        listVerbNew.append(token)\n",
        "        listVerbIdsNew.append(id)\n",
        "dctOpsLog.append('Dictionary contains ' + str(len(dct)) + ' terms (Nouns: ' + str(finalNosNouns) + ' / Adjs: ' + str(finalNosAdjs) + ' / Verbs: ' + str(finalNosVerbs) + ' / ImpKeywords: ' + str(finalNosImpKeywords) + ') after filtering out bad terms.')\n",
        "print(dctOpsLog[-1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4NNjJ5F4DPYo"
      },
      "source": [
        "# Creating gensim corpus out of processed reviews"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kvDzAYkBDQhf"
      },
      "source": [
        "corpus = [dct.doc2bow(line) for line in data_processed]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PQQ5NNRlDSIv"
      },
      "source": [
        "# Saving necessary pickles for further use in the pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TQvoHGhdDUlC"
      },
      "source": [
        "dct.save('dataOCM/02_LDA/LDA_02_Preprocessing_Dictionary.dictionary')\n",
        "pickle.dump(corpus, open('dataOCM/02_LDA/LDA_02_Preprocessing_Corpus.corpus', 'wb'))\n",
        "pickle.dump(keywordsConstructAllNew, open('dataOCM/02_LDA/LDA_02_Preprocessing_keywordsConstructAllNew.list', 'wb'))\n",
        "pickle.dump(keywordsConstructAllIDsInDctNew, open('dataOCM/02_LDA/LDA_02_Preprocessing_keywordsConstructAllIDsInDctNew.list', 'wb'))\n",
        "pickle.dump(listNounNew, open('dataOCM/02_LDA/LDA_02_Preprocessing_listNounNew.list', 'wb'))\n",
        "pickle.dump(listAdjNew, open('dataOCM/02_LDA/LDA_02_Preprocessing_listAdjNew.list', 'wb'))\n",
        "pickle.dump(listVerbNew, open('dataOCM/02_LDA/LDA_02_Preprocessing_listVerbNew.list', 'wb'))\n",
        "pickle.dump(listNounIdsNew, open('dataOCM/02_LDA/LDA_02_Preprocessing_listNounIdsNew.list', 'wb'))\n",
        "pickle.dump(listAdjIdsNew, open('dataOCM/02_LDA/LDA_02_Preprocessing_listAdjIdsNew.list', 'wb'))\n",
        "pickle.dump(listVerbIdsNew, open('dataOCM/02_LDA/LDA_02_Preprocessing_listVerbIdsNew.list', 'wb'))\n",
        "pickle.dump(dctOpsLog, open('dataOCM/02_LDA/LDA_02_Preprocessing_dctOpsLog.list', 'wb'))\n",
        "\n",
        "# Takes max 5 hours"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}